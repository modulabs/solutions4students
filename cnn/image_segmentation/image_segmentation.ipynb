{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation\n",
    "\n",
    "### 다음과 같은 일반적인 workflow로 진행\n",
    "1. Visualize data/perform some exploratory data analysis\n",
    "2. Set up data pipeline and preprocessing\n",
    "3. Build model\n",
    "4. Train model\n",
    "5. Evaluate model\n",
    "6. Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 설명\n",
    "\n",
    "### Task\n",
    "* GIANA dataset으로 위내시경 이미지에서 용종을 segmentation 해보자.\n",
    "* 데이터 불러오기를 제외한 딥러닝 트레이닝 과정을 직접 구현해보는 것이 목표 입니다.\n",
    "* This code is borrowed from [TensorFlow tutorials/Image Segmentation](https://github.com/tensorflow/models/blob/master/samples/outreach/blogs/segmentation_blogpost/image_segmentation.ipynb) which is made of `tf.keras.layers` and `tf.enable_eager_execution()`.\n",
    "* You can see the detail description [tutorial link](https://github.com/tensorflow/models/blob/master/samples/outreach/blogs/segmentation_blogpost/image_segmentation.ipynb)  \n",
    "\n",
    "### Dataset\n",
    "* I use below dataset instead of [carvana-image-masking-challenge dataset](https://www.kaggle.com/c/carvana-image-masking-challenge/rules) in TensorFlow Tutorials which is a kaggle competition dataset.\n",
    "  * carvana-image-masking-challenge dataset: Too large dataset (14GB)\n",
    "* [Gastrointestinal Image ANAlys Challenges (GIANA)](https://giana.grand-challenge.org) Dataset (345MB)\n",
    "  * Train data: 300 images with RGB channels (bmp format)\n",
    "  * Train lables: 300 images with 1 channels (bmp format)\n",
    "  * Image size: 574 x 500\n",
    "* Training시 **image size는 256**으로 resize\n",
    "\n",
    "### Baseline code\n",
    "* Dataset: train, test로 split\n",
    "* Input data shape: (`batch_size`, 256, 256, 3)\n",
    "* Output data shape: (`batch_size`, 256, 256, 1)\n",
    "* Architecture: \n",
    "  * 간단한 Encoder-Decoder 구조\n",
    "  * U-Net 구조\n",
    "  * [`tf.keras.layers`](https://www.tensorflow.org/api_docs/python/tf/keras/layers) 사용\n",
    "* Training\n",
    "  * `tf.data.Dataset` 사용\n",
    "  * `model.fit()` 사용 for weight update\n",
    "* Evaluation\n",
    "  * MeanIOU: Image Segmentation에서 많이 쓰이는 evaluation measure\n",
    "  * tf.version 1.13 API: [`tf.metrics.mean_iou`](https://www.tensorflow.org/api_docs/python/tf/metrics/mean_iou)\n",
    "    * `tf.enable_eager_execution()`이 작동하지 않음\n",
    "    * 따라서 예전 방식대로 `tf.Session()`을 이용하여 작성하거나 아래와 같이 2.0 version으로 작성하여야 함\n",
    "  * tf.version 2.0 API: [`tf.keras.metrics.MeanIoU`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/metrics/MeanIoU)\n",
    "\n",
    "### Try some techniques\n",
    "* Change model architectures (Custom model)\n",
    "  * Try another models (Unet 모델)\n",
    "* Various regularization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import colab modules for Google Colab (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if necessary\n",
    "\n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_colab = False\n",
    "assert use_colab in [True, False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import base modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "mpl.rcParams['figure.figsize'] = (12,12)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import losses\n",
    "from tensorflow.python.keras import models\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train = True\n",
    "\n",
    "model_name = 'ed_model'\n",
    "assert model_name in ['ed_model', 'u-net']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 수집 및 Visualize\n",
    "\n",
    "### Download data\n",
    "\n",
    "이 프로젝트는 [Giana Dataset](https://giana.grand-challenge.org/Dates/)을 이용하여 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfortunately you cannot downlaod GIANA dataset from website\n",
    "# So I upload zip file on my dropbox\n",
    "# if you want to download from my dropbox uncomment below  \n",
    "if use_colab:\n",
    "  DATASET_PATH='./gdrive/My Drive/datasets/sd_train'\n",
    "else:\n",
    "  DATASET_PATH='../../datasets/sd_train'\n",
    "\n",
    "if not os.path.isdir(DATASET_PATH):\n",
    "  os.makedirs(DATASET_PATH)\n",
    "  \n",
    "  import urllib.request\n",
    "  u = urllib.request.urlopen(url='https://www.dropbox.com/s/1a11bw6zrm6bb77/sd_train.zip?dl=1')\n",
    "  data = u.read()\n",
    "  u.close()\n",
    " \n",
    "  with open('sd_train.zip', \"wb\") as f :\n",
    "    f.write(data)\n",
    "  print('Data has been downloaded')\n",
    "  \n",
    "  shutil.move(os.path.join('sd_train.zip'), os.path.join(DATASET_PATH))\n",
    "  file_path = os.path.join(DATASET_PATH, 'sd_train.zip')\n",
    "  \n",
    "  import zipfile\n",
    "  zip_ref = zipfile.ZipFile(file_path, 'r')\n",
    "  zip_ref.extractall(DATASET_PATH)\n",
    "  zip_ref.close()\n",
    "  print('Data has been extracted.')\n",
    "  \n",
    "else:\n",
    "  print('Data has already been downloaded and extracted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join(DATASET_PATH, 'sd_train')\n",
    "\n",
    "img_dir = os.path.join(dataset_dir, \"train\")\n",
    "label_dir = os.path.join(dataset_dir, \"train_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_filenames = [os.path.join(img_dir, filename) for filename in os.listdir(img_dir)]\n",
    "x_train_filenames.sort()\n",
    "y_train_filenames = [os.path.join(label_dir, filename) for filename in os.listdir(label_dir)]\n",
    "y_train_filenames.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_filenames, x_test_filenames, y_train_filenames, y_test_filenames = \\\n",
    "                    train_test_split(x_train_filenames, y_train_filenames, test_size=0.2, random_state=219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_examples = len(x_train_filenames)\n",
    "num_test_examples = len(x_test_filenames)\n",
    "\n",
    "print(\"Number of training examples: {}\".format(num_train_examples))\n",
    "print(\"Number of test examples: {}\".format(num_test_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "\n",
    "데이터 셋에서 5장 (`display_num`)의 이미지를 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_num = 5\n",
    "\n",
    "r_choices = np.random.choice(num_train_examples, display_num)\n",
    "\n",
    "plt.figure(figsize=(10, 15))\n",
    "for i in range(0, display_num * 2, 2):\n",
    "  img_num = r_choices[i // 2]\n",
    "  x_pathname = x_train_filenames[img_num]\n",
    "  y_pathname = y_train_filenames[img_num]\n",
    "  \n",
    "  plt.subplot(display_num, 2, i + 1)\n",
    "  plt.imshow(Image.open(x_pathname))\n",
    "  plt.title(\"Original Image\")\n",
    "  \n",
    "  example_labels = Image.open(y_pathname)\n",
    "  label_vals = np.unique(example_labels)\n",
    "  \n",
    "  plt.subplot(display_num, 2, i + 2)\n",
    "  plt.imshow(example_labels)\n",
    "  plt.title(\"Masked Image\")\n",
    "  \n",
    "plt.suptitle(\"Examples of Images and their Masks\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline and preprocessing 만들기\n",
    "\n",
    "### Set up hyper-parameters\n",
    "\n",
    "Hyper-parameter를 셋팅해보자. 이미지 사이즈, 배치 사이즈 등 training parameter들을 셋팅해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "image_size = 256\n",
    "img_shape = (image_size, image_size, 3)\n",
    "batch_size = 8\n",
    "max_epochs = 10\n",
    "print_steps = 10\n",
    "save_epochs = 1\n",
    "\n",
    "if use_colab:\n",
    "  checkpoint_dir = train_dir ='./gdrive/My Drive/train_ckpt/segmentation/exp1'\n",
    "  if not os.path.isdir(train_dir):\n",
    "    os.makedirs(train_dir)\n",
    "else:\n",
    "  checkpoint_dir = train_dir = 'train/exp1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build our input pipeline with `tf.data`\n",
    "\n",
    "Input data pipeline을 만들기 가장 좋은 방법은 [**tf.data**](https://www.tensorflow.org/guide/datasets) (링크 참조) 를 사용하는 것이다. `tf.data` API 를 잘 읽어보자.\n",
    "\n",
    "\n",
    "#### Our input pipeline will consist of the following steps:\n",
    "\n",
    "TensorFlow segmentation tutorial input pipeline 참고 하였음.\n",
    "\n",
    "\n",
    ">1. Read the bytes of the file in from the filename - for both the image and the label. Recall that our labels are actually images with each pixel annotated as car or background (1, 0). \n",
    ">2. Decode the bytes into an image format\n",
    ">3. Apply image transformations: (optional, according to input parameters)\n",
    ">  * `resize` - Resize our images to a standard size (as determined by eda or computation/memory restrictions)\n",
    ">    * The reason why this is optional is that U-Net is a fully convolutional network (e.g. with no fully connected units) and is thus not dependent on the input size. However, if you choose to not resize the images, you must use a batch size of 1, since you cannot batch variable image size together\n",
    ">    * Alternatively, you could also bucket your images together and resize them per mini-batch to avoid resizing images as much, as resizing may affect your performance through interpolation, etc.\n",
    ">  * `hue_delta` - Adjusts the hue of an RGB image by a random factor. This is only applied to the actual image (not our label image). The `hue_delta` must be in the interval `[0, 0.5]` \n",
    ">  * `horizontal_flip` - flip the image horizontally along the central axis with a 0.5 probability. This transformation must be applied to both the label and the actual image. \n",
    ">  * `width_shift_range` and `height_shift_range` are ranges (as a fraction of total width or height) within which to randomly translate the image either horizontally or vertically. This transformation must be applied to both the label and the actual image. \n",
    ">  * `rescale` - rescale the image by a certain factor, e.g. 1/ 255.\n",
    ">4. Shuffle the data, repeat the data (so we can iterate over it multiple times across epochs), batch the data, then prefetch a batch (for efficiency)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we do these image transformations?\n",
    "\n",
    "Data augmentation은 딥러닝을 이용한 이미지 처리분야 (classification, detection, segmentation 등) 에서 널리 쓰이는 테크닉이다. 자세한 내용은 아래 TensorFlow 공식 예제 링크로 대체한다.\n",
    "\n",
    "> This is known as **data augmentation**. Data augmentation \"increases\" the amount of training data by augmenting them via a number of random transformations. During training time, our model would never see twice the exact same picture. This helps prevent [overfitting](https://developers.google.com/machine-learning/glossary/#overfitting) and helps the model generalize better to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing each pathname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_pathnames(fname, label_path):\n",
    "  # We map this function onto each pathname pair\n",
    "  img_str = tf.read_file(fname)\n",
    "  img = tf.image.decode_bmp(img_str, channels=3)\n",
    "\n",
    "  label_img_str = tf.read_file(label_path)\n",
    "  label_img = tf.image.decode_bmp(label_img_str, channels=1)\n",
    "  \n",
    "  resize = [image_size, image_size]\n",
    "  img = tf.image.resize_images(img, resize)\n",
    "  label_img = tf.image.resize_images(label_img, resize)\n",
    "  \n",
    "  scale = 1 / 255.\n",
    "  img = tf.cast(img, dtype=tf.float32) * scale\n",
    "  label_img = tf.cast(label_img, dtype=tf.float32) * scale\n",
    "  \n",
    "  return img, label_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shifting the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_img(output_img, label_img, width_shift_range, height_shift_range):\n",
    "  \"\"\"This fn will perform the horizontal or vertical shift\"\"\"\n",
    "  if width_shift_range or height_shift_range:\n",
    "      if width_shift_range:\n",
    "        width_shift_range = tf.random_uniform([], \n",
    "                                              -width_shift_range * img_shape[1],\n",
    "                                              width_shift_range * img_shape[1])\n",
    "      if height_shift_range:\n",
    "        height_shift_range = tf.random_uniform([],\n",
    "                                               -height_shift_range * img_shape[0],\n",
    "                                               height_shift_range * img_shape[0])\n",
    "      # Translate both \n",
    "      output_img = tfcontrib.image.translate(output_img,\n",
    "                                             [width_shift_range, height_shift_range])\n",
    "      label_img = tfcontrib.image.translate(label_img,\n",
    "                                             [width_shift_range, height_shift_range])\n",
    "  return output_img, label_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flipping the image randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_img(horizontal_flip, tr_img, label_img):\n",
    "  if horizontal_flip:\n",
    "    flip_prob = tf.random_uniform([], 0.0, 1.0)\n",
    "    tr_img, label_img = tf.cond(tf.less(flip_prob, 0.5),\n",
    "                                lambda: (tf.image.flip_left_right(tr_img), tf.image.flip_left_right(label_img)),\n",
    "                                lambda: (tr_img, label_img))\n",
    "  return tr_img, label_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assembling our transformations into our augment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _augment(img,\n",
    "             label_img,\n",
    "             resize=None,  # Resize the image to some size e.g. [256, 256]\n",
    "             scale=1,  # Scale image e.g. 1 / 255.\n",
    "             hue_delta=0,  # Adjust the hue of an RGB image by random factor\n",
    "             horizontal_flip=False,  # Random left right flip,\n",
    "             width_shift_range=0,  # Randomly translate the image horizontally\n",
    "             height_shift_range=0):  # Randomly translate the image vertically \n",
    "  if resize is not None:\n",
    "    # Resize both images\n",
    "    label_img = tf.image.resize_images(label_img, resize)\n",
    "    img = tf.image.resize_images(img, resize)\n",
    "  \n",
    "  if hue_delta:\n",
    "    img = tf.image.random_hue(img, hue_delta)\n",
    "  \n",
    "  img, label_img = flip_img(horizontal_flip, img, label_img)\n",
    "  img, label_img = shift_img(img, label_img, width_shift_range, height_shift_range)\n",
    "  label_img = tf.cast(label_img, dtype=tf.float32) * scale\n",
    "  img = tf.cast(img, dtype=tf.float32) * scale\n",
    "  return img, label_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_dataset(filenames,\n",
    "                         labels,\n",
    "                         preproc_fn=functools.partial(_augment),\n",
    "                         threads=5,\n",
    "                         batch_size=batch_size,\n",
    "                         is_train=True):\n",
    "  num_x = len(filenames)\n",
    "  # Create a dataset from the filenames and labels\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "  # Map our preprocessing function to every element in our dataset, taking\n",
    "  # advantage of multithreading\n",
    "  dataset = dataset.map(_process_pathnames, num_parallel_calls=threads)\n",
    "  \n",
    "  if is_train:\n",
    "    #if preproc_fn.keywords is not None and 'resize' not in preproc_fn.keywords:\n",
    "    #  assert batch_size == 1, \"Batching images must be of the same size\"\n",
    "    dataset = dataset.map(preproc_fn, num_parallel_calls=threads)\n",
    "    dataset = dataset.shuffle(num_x * 10)\n",
    "  \n",
    "  dataset = dataset.batch(batch_size)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up train and test datasets\n",
    "Note that we apply image augmentation to our training dataset but not our validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_baseline_dataset(x_train_filenames,\n",
    "                                     y_train_filenames)\n",
    "test_dataset = get_baseline_dataset(x_test_filenames,\n",
    "                                    y_test_filenames,\n",
    "                                    is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot some train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_dataset.take(1):\n",
    "  # Running next element in our graph will produce a batch of images\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  img = images[0]\n",
    "\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.imshow(img)\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.imshow(labels[0, :, :, 0])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "해당 프로젝트는 두 개의 네트워크를 만들어보는 것이 목표이다.\n",
    "* Encoder-Decoder 스타일의 네트워크\n",
    "* [U-Net](https://arxiv.org/abs/1505.04597)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder architecture\n",
    "\n",
    "#### Encoder\n",
    "* 다음과 같은 구조로 Encoder로 만들어보자.\n",
    "* `input data`의 shape이 다음과 같이 되도록 네트워크를 구성해보자\n",
    "  * inputs = [batch_size, 256, 256, 3]\n",
    "  * conv1 = [batch_size, 128, 128, 32]\n",
    "  * conv2 = [batch_size, 64, 64, 64]\n",
    "  * conv3 = [batch_size, 32, 32, 128]\n",
    "  * outputs = [batch_size, 16, 16, 256]\n",
    "* Convolution - Normalization - Activation 등의 조합을 다양하게 생각해보자.\n",
    "* Pooling을 쓸지 Convolution with stride=2 로 할지 잘 생각해보자.\n",
    "* `tf.keras.Sequential()`을 이용하여 만들어보자.\n",
    "  \n",
    "#### Decoder\n",
    "* Encoder의 mirror 형태로 만들어보자.\n",
    "* `input data`의 shape이 다음과 같이 되도록 네트워크를 구성해보자\n",
    "  * inputs = encoder의 outputs = [batch_size, 16, 16, 256]\n",
    "  * conv_transpose1 = [batch_size, 32, 32, 128]\n",
    "  * conv_transpose2 = [batch_size, 64, 64, 64]\n",
    "  * conv_transpose3 = [batch_size, 128, 128, 32]\n",
    "  * outputs = [batch_size, 256, 256, 1]\n",
    "* `tf.keras.Sequential()`을 이용하여 만들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'ed_model':\n",
    "  encoder = tf.keras.Sequential(name='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'ed_model':\n",
    "  # inputs: [batch_size, 256, 256, 3]\n",
    "  encoder.add(layers.Conv2D(32, (3, 3), padding='same'))\n",
    "  encoder.add(layers.BatchNormalization())\n",
    "  encoder.add(layers.Activation('relu'))\n",
    "  encoder.add(layers.MaxPool2D()) # conv1: [batch_size, 128, 128, 32]\n",
    "\n",
    "  encoder.add(layers.Conv2D(64, (3, 3), padding='same'))\n",
    "  encoder.add(layers.BatchNormalization())\n",
    "  encoder.add(layers.Activation('relu'))\n",
    "  encoder.add(layers.MaxPool2D()) # conv2: [batch_size, 64, 64, 64]\n",
    "\n",
    "  encoder.add(layers.Conv2D(128, (3, 3), padding='same'))\n",
    "  encoder.add(layers.BatchNormalization())\n",
    "  encoder.add(layers.Activation('relu'))\n",
    "  encoder.add(layers.MaxPool2D()) # outputs: [batch_size, 32, 32, 128]\n",
    "\n",
    "  encoder.add(layers.Conv2D(256, (3, 3), padding='same'))\n",
    "  encoder.add(layers.BatchNormalization())\n",
    "  encoder.add(layers.Activation('relu'))\n",
    "  encoder.add(layers.MaxPool2D()) # outputs: [batch_size, 16, 16, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'ed_model':\n",
    "  bottleneck = encoder(tf.random.normal([3, 256, 256, 3]))\n",
    "  print(bottleneck.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'ed_model':\n",
    "  decoder = tf.keras.Sequential(name='decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'ed_model':\n",
    "  # inputs: [batch_size, 16, 16, 256]\n",
    "  decoder.add(layers.Conv2DTranspose(128, (2, 2), strides=2, padding='same'))\n",
    "  decoder.add(layers.BatchNormalization())\n",
    "  decoder.add(layers.Activation('relu')) # conv_transpose1: [batch_size, 32, 32, 128]\n",
    "\n",
    "  decoder.add(layers.Conv2DTranspose(64, (2, 2), strides=2, padding='same'))\n",
    "  decoder.add(layers.BatchNormalization())\n",
    "  decoder.add(layers.Activation('relu')) # conv_transpose2: [batch_size, 64, 64, 64]\n",
    "\n",
    "  decoder.add(layers.Conv2DTranspose(128, (2, 2), strides=2, padding='same'))\n",
    "  decoder.add(layers.BatchNormalization())\n",
    "  decoder.add(layers.Activation('relu')) # conv_transpose3: [batch_size, 128, 128, 32]\n",
    "\n",
    "  decoder.add(layers.Conv2DTranspose(1, (2, 2), strides=2, padding='same'))\n",
    "  decoder.add(layers.BatchNormalization())\n",
    "  decoder.add(layers.Activation('sigmoid')) # outputs: [batch_size, 256, 256, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'ed_model':\n",
    "  predictions = decoder(bottleneck)\n",
    "  print(predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a encoder-decocer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'ed_model':\n",
    "  ed_model = tf.keras.Sequential()\n",
    "  ed_model.add(encoder)\n",
    "  ed_model.add(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net architecture\n",
    "\n",
    "<img src='https://user-images.githubusercontent.com/11681225/58005153-fd934300-7b1f-11e9-9ad8-a0e9186e751c.png' width=\"800\">\n",
    "\n",
    "아래는 U-Net 만들 때 참고하면 좋은 TensorFlow tutorial 설명이다.\n",
    "\n",
    ">We'll build the U-Net model. U-Net is especially good with segmentation tasks because it can localize well to provide high resolution segmentation masks. In addition, it works well with small datasets and is relatively robust against overfitting as the training data is in terms of the number of patches within an image, which is much larger than the number of training images itself. Unlike the original model, we will add batch normalization to each of our blocks. \n",
    "\n",
    ">The Unet is built with an encoder portion and a decoder portion. The encoder portion is composed of a linear stack of [`Conv`](https://developers.google.com/machine-learning/glossary/#convolution), `BatchNorm`, and [`Relu`](https://developers.google.com/machine-learning/glossary/#ReLU) operations followed by a [`MaxPool`](https://developers.google.com/machine-learning/glossary/#pooling). Each `MaxPool` will reduce the spatial resolution of our feature map by a factor of 2. We keep track of the outputs of each block as we feed these high resolution feature maps with the decoder portion. The Decoder portion is comprised of UpSampling2D, Conv, BatchNorm, and Relus. Note that we concatenate the feature map of the same size on the decoder side. Finally, we add a final Conv operation that performs a convolution along the channels for each individual pixel (kernel size of (1, 1)) that outputs our final segmentation mask in grayscale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `tf.keras` Functional API\n",
    "\n",
    "U-Net은 Encoder-Decoder 구조와는 달리 해당 레이어의 outputs이 바로 다음 레이어의 inputs이 되지 않는다. 이럴때는 `tf.keras.Sequential()`을 쓸 수가 없다. Sequential 구조가 아닌 네트워크를 만들 때 쓸 수 있는 API 가 바로 `tf.keras` functional API 이다. 자세한 설명은 다음 [문서](https://keras.io/getting-started/functional-api-guide/)를 참고 하면 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'u-net':\n",
    "  class Conv(tf.keras.Model):\n",
    "    def __init__(self, num_filters, kernel_size):\n",
    "      super(Conv, self).__init__()\n",
    "      self.conv = layers.Conv2D(num_filters, kernel_size, padding='same')\n",
    "      self.bn = layers.BatchNormalization()\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "      x = self.conv(inputs)\n",
    "      x = self.bn(x, training=training)\n",
    "      x = tf.nn.relu(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'u-net':\n",
    "  class ConvBlock(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "      super(ConvBlock, self).__init__()\n",
    "      self.conv1 = Conv(num_filters, 3)\n",
    "      self.conv2 = Conv(num_filters, 3)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "      encoder = self.conv1(inputs, training=training)\n",
    "      encoder = self.conv2(encoder, training=training)\n",
    "\n",
    "      return encoder\n",
    "\n",
    "\n",
    "  class EncoderBlock(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "      super(EncoderBlock, self).__init__()\n",
    "      self.conv_block = ConvBlock(num_filters)\n",
    "      self.encoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "      encoder = self.conv_block(inputs, training=training)\n",
    "      encoder_pool = self.encoder_pool(encoder)\n",
    "\n",
    "      return encoder_pool, encoder\n",
    "\n",
    "\n",
    "  class DecoderBlock(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "      super(DecoderBlock, self).__init__()\n",
    "      self.convT = layers.Conv2DTranspose(num_filters, 2, strides=2, padding='same')\n",
    "      self.bn = layers.BatchNormalization()\n",
    "      self.conv_block = ConvBlock(num_filters)\n",
    "\n",
    "    def call(self, input_tensor, concat_tensor, training=True):\n",
    "      decoder = self.convT(input_tensor)\n",
    "      decoder = tf.concat([decoder, concat_tensor], axis=-1)\n",
    "      decoder = self.bn(decoder, training=training)\n",
    "      decoder = tf.nn.relu(decoder)\n",
    "      decoder = self.conv_block(decoder, training=training)\n",
    "\n",
    "      return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'u-net':\n",
    "  class UNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "      super(UNet, self).__init__()\n",
    "      self.encoder_block1 = EncoderBlock(32)\n",
    "      self.encoder_block2 = EncoderBlock(64)\n",
    "      self.encoder_block3 = EncoderBlock(128)\n",
    "      self.encoder_block4 = EncoderBlock(256)\n",
    "\n",
    "      self.center = ConvBlock(512)\n",
    "\n",
    "      self.decoder_block4 = DecoderBlock(256)\n",
    "      self.decoder_block3 = DecoderBlock(128)\n",
    "      self.decoder_block2 = DecoderBlock(64)\n",
    "      self.decoder_block1 = DecoderBlock(32)\n",
    "\n",
    "      self.output_conv = layers.Conv2D(1, 1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "      encoder1_pool, encoder1 = self.encoder_block1(inputs)\n",
    "      encoder2_pool, encoder2 = self.encoder_block2(encoder1_pool)\n",
    "      encoder3_pool, encoder3 = self.encoder_block3(encoder2_pool)\n",
    "      encoder4_pool, encoder4 = self.encoder_block4(encoder3_pool)\n",
    "\n",
    "      center = self.center(encoder4_pool)\n",
    "\n",
    "      decoder4 = self.decoder_block4(center, encoder4)\n",
    "      decoder3 = self.decoder_block3(decoder4, encoder3)\n",
    "      decoder2 = self.decoder_block2(decoder3, encoder2)\n",
    "      decoder1 = self.decoder_block1(decoder2, encoder1)\n",
    "\n",
    "      outputs = self.output_conv(decoder1)\n",
    "\n",
    "      return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a U-Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'u-net':\n",
    "  unet_model = UNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining custom metrics and loss functions\n",
    "\n",
    "Defining loss and metric functions are simple with Keras. Simply define a function that takes both the True labels for a given example and the Predicted labels for the same given example.\n",
    "\n",
    "Dice loss is a metric that measures overlap. More info on optimizing for Dice coefficient (our dice loss) can be found in the [paper](http://campar.in.tum.de/pub/milletari2016Vnet/milletari2016Vnet.pdf), where it was introduced.\n",
    "\n",
    "We use dice loss here because it performs better at class imbalanced problems by design. In addition, maximizing the dice coefficient and IoU metrics are the actual objectives and goals of our segmentation task. Using cross entropy is more of a proxy which is easier to maximize. Instead, we maximize our objective directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coeff(y_true, y_pred):\n",
    "  smooth = 1.\n",
    "  # Flatten\n",
    "  y_true_f = tf.reshape(y_true, [-1])\n",
    "  y_pred_f = tf.reshape(y_pred, [-1])\n",
    "  intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "  score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred):\n",
    "  loss = 1 - dice_coeff(y_true, y_pred)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll use a specialized loss function that combines binary cross entropy and our dice loss. This is based on [individuals who competed within this competition obtaining better results empirically](https://www.kaggle.com/c/carvana-image-masking-challenge/discussion/40199). Try out your own custom losses to measure performance (e.g. bce + log(dice_loss), only bce, etc.)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_dice_loss(y_true, y_pred):\n",
    "  loss = tf.reduce_mean(losses.binary_crossentropy(y_true, y_pred)) + dice_loss(y_true, y_pred)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'ed_model':\n",
    "  print('select the Encoder-Decoder model')\n",
    "  model = ed_model\n",
    "\n",
    "if model_name == 'u-net':\n",
    "  print('select the U-Net model')\n",
    "  model = unet_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=bce_dice_loss, metrics=[dice_loss])\n",
    "predictions = model(tf.random.normal([batch_size, 256, 256, 3]))\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not tf.gfile.Exists(checkpoint_dir):\n",
    "  tf.gfile.MakeDirs(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "if is_train:\n",
    "  checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                   model=model)\n",
    "else:\n",
    "    checkpoint = tf.train.Checkpoint(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define print function\n",
    "def print_images():\n",
    "  for test_images, test_labels in test_dataset.take(1):\n",
    "    predictions = model(test_images, training=False)\n",
    "        \n",
    "    plt.figure(figsize=(10, 20))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(test_images[0,: , :, :])\n",
    "    plt.title(\"Input image\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(test_labels[0, :, :, 0])\n",
    "    plt.title(\"Actual Mask\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(predictions[0, :, :, 0])\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training 첫번째 방법 `model.fit()` 함수 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset, epochs=max_epochs,\n",
    "          steps_per_epoch=num_train_examples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print sample image after training\n",
    "print_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weight\n",
    "checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training 두 번째 방법 `tf.GradientTape()`을 이용하여 직접 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Start Training.')\n",
    "num_batches_per_epoch = num_train_examples // batch_size\n",
    "global_step = 0\n",
    "# save loss values for plot\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "  \n",
    "  for step, (images, labels) in enumerate(train_dataset):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "      predictions = model(images, training=True)\n",
    "      loss = bce_dice_loss(labels, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.variables))\n",
    "    \n",
    "    if global_step % print_steps == 0:\n",
    "      clear_output(wait=True)\n",
    "      epochs = epoch + step / float(num_batches_per_epoch)\n",
    "      duration = time.time() - start_time\n",
    "      examples_per_sec = batch_size  / float(duration)\n",
    "      print(\"Epochs: {:.2f} global_step: {} loss: {:.3f} ({:.2f} examples/sec; {:.3f} sec/batch)\".format(\n",
    "                epochs, global_step, loss, examples_per_sec, duration))\n",
    "\n",
    "      loss_history.append([epochs, loss])\n",
    "\n",
    "      # print sample image\n",
    "      print_images()\n",
    "\n",
    "  # saving (checkpoint) the model periodically\n",
    "  if (epoch+1) % save_epochs == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "print('Training Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = np.asarray(loss_history)\n",
    "plt.plot(loss_history[:,0], loss_history[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_train:\n",
    "  # restoring the latest checkpoint in checkpoint_dir\n",
    "  status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_iou(y_true, y_pred):\n",
    "  # Flatten\n",
    "  y_true_f = tf.keras.layers.Flatten()(y_true)\n",
    "  y_pred_f = tf.keras.layers.Flatten()(y_pred)\n",
    "  \n",
    "  y_true_f = tf.to_int32(tf.round(y_true_f))\n",
    "  y_pred_f = tf.to_int32(tf.round(y_pred_f))\n",
    "  \n",
    "  intersection = tf.reduce_sum(y_true_f * y_pred_f, axis=1)\n",
    "  union = tf.reduce_sum(tf.clip_by_value(y_true_f + y_pred_f, 0, 1), axis=1)\n",
    "  \n",
    "  mean_iou = tf.reduce_mean(intersection/union)\n",
    "  \n",
    "  return mean_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = tf.keras.metrics.Mean(\"mean_iou\")\n",
    "\n",
    "for images, labels in test_dataset.take(3):\n",
    "  predictions = model(images, training=False)\n",
    "  m = mean_iou(labels, predictions)\n",
    "  mean(m)\n",
    "\n",
    "print(\"mean_iou: {}\".format(mean.result().numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
