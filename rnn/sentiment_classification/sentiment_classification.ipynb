{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification\n",
    "\n",
    "### Task\n",
    "* IMDB 영화사이트에서 50000개의 영화평을 가지고 positive/negative인지 구분해보자.\n",
    "* 데이터 불러오기를 제외한 딥러닝 트레이닝 과정을 직접 구현해보는 것이 목표 입니다.\n",
    "\n",
    "### Dataset\n",
    "* [IMDB datasets](https://www.imdb.com/interfaces/)\n",
    "\n",
    "### Base code\n",
    "* Dataset: train, test로 split (validation data는 `model.fit`할 때 임의로 배정)\n",
    "* Input data shape: (`batch_size`, `max_sequence_length`)\n",
    "* Output data shape: (`batch_size`, 1)\n",
    "* Architecture:\n",
    "  * RNN을 이용한 간단한 classification 모델 가이드\n",
    "  * `Embedding` - `SimpleRNN` - `Dense (with Sigmoid)`\n",
    "  * [`tf.keras.layers`](https://www.tensorflow.org/api_docs/python/tf/keras/layers) 사용\n",
    "* Training\n",
    "  * `model.fit` 사용\n",
    "* Evaluation\n",
    "  * `model.evaluate` 사용 for test dataset\n",
    "\n",
    "### Try some techniques\n",
    "* Training-epochs 조절\n",
    "* Change model architectures (Custom model)\n",
    "  * Use another cells (LSTM, GRU, etc.)\n",
    "  * Use dropout layers\n",
    "* Embedding size 조절\n",
    "  * 또는 one-hot vector로 학습\n",
    "* Number of words in the vocabulary 변화\n",
    "* `pad` 옵션 변화\n",
    "* Data augmentation (if possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어처리에 관한 work flow\n",
    "\n",
    "The flowchart of the algorithm is roughly:\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/11681225/46912373-d2a3a800-cfae-11e8-8201-ef17b65834f5.png\" alt=\"natural_language_flowchart\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import base modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:05:42.026345Z",
     "start_time": "2019-03-04T06:05:40.302942Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "from tensorflow.python.keras import layers\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "* IMDB에서 다운받은 총 50000개의 영화평을 사용한다.\n",
    "* `tf.keras.datasets`에 이미 잘 가공된 데이터 셋이 있으므로 쉽게 다운받아 사용할 수 있다.\n",
    "* 원래는 text 데이터이지만 `tf.keras.datasets.imdb`는 이미 Tokenizing이 되어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:05:42.083083Z",
     "start_time": "2019-03-04T06:05:42.075925Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load training and eval data from tf.keras\n",
    "imdb = tf.keras.datasets.imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "train_labels = train_labels.astype(np.float64)\n",
    "test_labels = test_labels.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:05:48.627470Z",
     "start_time": "2019-03-04T06:05:48.619679Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Train-set size: \", len(train_data))\n",
    "print(\"Test-set size:  \", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sequence length: {}\".format(len(train_data[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Label정보\n",
    "  * 0.0 for a negative sentiment\n",
    "  * 1.0 for a positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:05:51.490655Z",
     "start_time": "2019-03-04T06:05:51.486122Z"
    }
   },
   "outputs": [],
   "source": [
    "# positive sample\n",
    "index = 1\n",
    "print(\"text: {}\\n\".format(train_data[index]))\n",
    "print(\"label: {}\".format(train_labels[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:05:52.735239Z",
     "start_time": "2019-03-04T06:05:52.731203Z"
    }
   },
   "outputs": [],
   "source": [
    "# negative sample\n",
    "index = 200\n",
    "print(\"text: {}\\n\".format(train_data[index]))\n",
    "print(\"label: {}\".format(train_labels[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "\n",
    "### Convert the integers back to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary mapping words to an integer index\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# The first indices are reserved\n",
    "word_index = {k:(v+3) for k,v in word_index.items()} \n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "  return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text data 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_review(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding and truncating data using pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seq_length = np.array([len(tokens) for tokens in list(train_data) + list(test_data)])\n",
    "train_seq_length = np.array([len(tokens) for tokens in train_data], dtype=np.int32)\n",
    "test_seq_length = np.array([len(tokens) for tokens in test_data], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(num_seq_length < max_seq_length) / len(num_seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `max_seq_length`을 256으로 설정하면 전체 데이터 셋의 70%를 커버할 수 있다.\n",
    "* 30% 정도의 데이터가 256 단어가 넘는 문장으로 이루어져 있다.\n",
    "* 보통 미리 정한 `max_seq_length`를 넘어가는 문장의 데이터는 *truncate* 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding 옵션은 두 가지가 있다.\n",
    "#pad = 'pre'\n",
    "pad = 'post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:07:16.640655Z",
     "start_time": "2019-03-04T06:07:16.607400Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_pad = pad_sequences(train_data,\n",
    "                               maxlen=max_seq_length,\n",
    "                               padding=pad,\n",
    "                               value=word_index[\"<PAD>\"])\n",
    "test_data_pad = pad_sequences(test_data,\n",
    "                              maxlen=max_seq_length,\n",
    "                              padding=pad,\n",
    "                              value=word_index[\"<PAD>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_pad.shape)\n",
    "print(test_data_pad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding data 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:07:59.449979Z",
     "start_time": "2019-03-04T06:07:58.334795Z"
    }
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "print(\"text: {}\\n\".format(decode_review(train_data[index])))\n",
    "print(\"token: {}\\n\".format(train_data[index]))\n",
    "print(\"pad: {}\".format(train_data_pad[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:08:04.189086Z",
     "start_time": "2019-03-04T06:08:04.184027Z"
    }
   },
   "outputs": [],
   "source": [
    "num_val_data = 5000\n",
    "val_data_pad = train_data_pad[:num_val_data]\n",
    "train_data_pad_partial = train_data_pad[num_val_data:]\n",
    "\n",
    "val_labels = train_labels[:num_val_data]\n",
    "train_labels_partial = train_labels[num_val_data:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:08:34.724505Z",
     "start_time": "2019-03-04T06:08:34.720495Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the hyperparameter set\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "max_epochs = 3\n",
    "embedding_size = 32\n",
    "vocab_size = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:10:43.571910Z",
     "start_time": "2019-03-04T06:10:43.567936Z"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer\n",
    "\n",
    "* embedding-layer는 전체 vocabulary의 갯수(num_words)로 이루어진 index가 `embedding_size`의 *dense vector* 로 변환되는 과정이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:10:48.761441Z",
     "start_time": "2019-03-04T06:10:48.753644Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "model.add(layers.Embedding(vocab_size, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main rnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:10:53.962777Z",
     "start_time": "2019-03-04T06:10:53.944541Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.add 를 통해 자유롭게 모델을 만들어보세요.\n",
    "# TODO\n",
    "model.add(layers.SimpleRNN(units=16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:11:09.533973Z",
     "start_time": "2019-03-04T06:11:09.523655Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "model.add(layers.Dense(1, activation=tf.nn.sigmoid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:11:14.728897Z",
     "start_time": "2019-03-04T06:11:14.721969Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:11:19.741901Z",
     "start_time": "2019-03-04T06:11:19.734665Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:11:57.988951Z",
     "start_time": "2019-03-04T06:11:29.719361Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(train_data_pad_partial,\n",
    "                    train_labels_partial,\n",
    "                    epochs=max_epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(val_data_pad, val_labels),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on Test-Set\n",
    "\n",
    "Now that the model has been trained we can calculate its classification accuracy on the test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:12:07.230716Z",
     "start_time": "2019-03-04T06:12:03.470948Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "results = model.evaluate(test_data_pad, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T05:56:34.572611Z",
     "start_time": "2019-03-04T05:56:34.116Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss\n",
    "print(\"loss value: {:.3f}\".format(results[0]))\n",
    "# accuracy\n",
    "print(\"accuracy value: {:.3f}\".format(results[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "553px",
    "left": "792px",
    "right": "61px",
    "top": "71px",
    "width": "375px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
